import numpy as np
import torch
import torch.nn as nn
from torch.nn import init
import torch.nn.functional as F

class ImpConGCN(nn.Module):
    """This module implements the vanilla GCN model."""

    def __init__(self, in_dim, out_dim, N=25, use_bn=False):
        super(ImpConGCN, self).__init__()

        if out_dim == 0:
            out_dim = in_dim

        self.Wq = nn.Linear(in_dim,out_dim,bias=False)
        self.Wk = nn.Linear(in_dim,out_dim,bias=False)
        self.conv = nn.Linear(in_dim,out_dim,bias=False)

        init.xavier_uniform_(self.conv.weight)
        init.xavier_uniform_(self.Wq.weight)
        init.xavier_uniform_(self.Wk.weight)

        self.use_bn = use_bn
        if self.use_bn: #Consider using Graph Norm here or instance norm
            self.bn = nn.BatchNorm2d(out_dim)


    def learn_adj(self, features, A):
        """We implement the algorithm that learns the adjacency matrix"""
        if len(A.shape) == 2:
            b, t, _, _ = features.shape
            A = torch.from_numpy(A).unsqueeze(0).unsqueeze(0) #[1 x 1 x n_nodes x n_nodes]
            A = A.repeat(b, t, 1, 1).to(features.device) #[b x time x n_nodes x n_nodes]

        # May remove normalize
        x1 = F.normalize(self.Wq(features), dim=-2) #[b x time x n_nodes x out_channel]
        x2 = F.normalize(self.Wk(features), dim=-2) #[b x time x n_nodes x out_channel]

        x1x2 = torch.matmul(x1,x2.transpose(-1,-2)) #[b x time x n_nodes x n_nodes]
        out = torch.tanh(x1x2) #[b x time x n_nodes x n_nodes]
        # out = torch.where(out > self.threshold, out, 0.) #[b x time x n_nodes x n_nodes]
        out = F.relu(out) #This sets the threshold to zero
        adj = torch.where(A == 0., out, A) #[b x time x n_nodes x n_nodes]

        filtered_x1x2 = torch.where(adj != 0., x1x2, -torch.inf) #[b x time x n_nodes x n_nodes]
        strength = F.softmax(filtered_x1x2, dim=-1) #[b x time x n_nodes x n_nodes]

        A = adj * strength #[b x time x n_nodes x n_nodes]

        return A.float()


    def forward(self, features, A):
        """
            features:   --dim(batch_size,time,num_nodes,in_channel)
            A:          --dim(num_nodes,num_nodes)
        """
        A = self.learn_adj(features, A) #[b x time x n_nodes x n_nodes]
        M = torch.eye(A.size(-1), dtype=torch.float).to(features.device) #for self-loop

        out = self.conv(features) #[b x time x n_nodes x out_channel]
        out = ((A * M).matmul(out) # This line is for self-loop
                    + (A * (1-M)).matmul(out)) #[b x time x out_channel x n_nodes]

        if self.use_bn:
            out = self.bn(out.transpose(1,2)) #[b x out_channel x time x n_nodes]
            out = out.transpose(1,2)#[b x time x out_channel x n_nodes]

        return out.transpose(-1,-2) #[b x time x n_nodes x out_channel]
